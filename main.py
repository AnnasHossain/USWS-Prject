import openaiimport configimport randomimport datetimeimport chromadbimport re# saving datas in our specific file (needs possibly to be changed to run code)chroma_client = chromadb.PersistentClient(path="....here put path of this project...")# create or retrieve the collection (database)try:    collection = chroma_client.create_collection(name="my_collection")except Exception as e:    collection = chroma_client.get_collection(name="my_collection")# use API key from the configuration file (needs possibly to be changed in api_key file to run code)openai.api_key = config.api_keymembers = ['Elias', 'Leon', 'Matteo']# Initialize memory stream (as a list first)conversation_history = []# topic (any topic can be chosen)topic = 'talk about methods for better learning'num_rounds = 3def generate_rating_prompt(conversation_history, members):    conversation_text = "\n".join(conversation_history)    prompt = f"Rate the contributions in the following conversation and provide an explanation for each rating:\n{conversation_text}\n\n"    for member in members:        prompt += f"Rate {member}'s overall contributions on a scale of 1 (good) to 10 (bad) and provide an explanation: "    return promptdef get_member_ratings(conversation_history, members):    prompt = generate_rating_prompt(conversation_history, members)    response = openai.Completion.create(        model="gpt-3.5-turbo-instruct",        prompt=prompt,        max_tokens=200    )    ratings_text = response.choices[0].text.strip()    ratings = {}    for member in members:        # search for "member: Rating - Explanation"        match = re.search(f"{member}: (\\d+|No rating found) - (.*?)$", ratings_text, re.MULTILINE)        if match:            rating = match.group(1)            explanation = match.group(2).strip()        else:            rating = "No rating found"            explanation = "No explanation provided"        ratings[member] = (rating, explanation)    return ratingsdef generate_explanation_prompt(members, ratings):    prompt = "Please explain why the following ratings were given:\n"    for member in members:        prompt += f"{member}: {ratings[member]} - Explanation: "    return promptdef get_explanations_for_ratings(members, ratings):    prompt = generate_explanation_prompt(members, ratings)    response = openai.Completion.create(        model="gpt-3.5-turbo-instruct",        prompt=prompt,        max_tokens=400    )    explanations_text = response.choices[0].text.strip()    explanations = {}    for line in explanations_text.split('\n'):        parts = line.split('-')        if len(parts) == 2 and parts[0].strip() in members:            explanations[parts[0].strip()] = parts[1].strip()    return explanations# generating high level questions based on conversation_summaries (for each person)def generate_aspects_reflection(conversation_summaries):    prompt = "Given only the information here {conversation_summaries}. What are the 3 most high-level questions of the subjects in the statements?"    for summary in conversation_summaries:        prompt += f"\n{summary['participant']}: {summary['conversation']}"    prompt += "\n\nQuestions:"    response = openai.Completion.create(        model="gpt-3.5-turbo-instruct",        prompt=prompt,        max_tokens=500    )    return response.choices[0].text.strip()# using the generated high-level reflections to generate answers (statements) by conversation_summaeriesdef generate_reflection_prompt(conversation_summaries):    # Generieren von Aspekten basierend auf den Gespr√§chszusammenfassungen bzgl. der Fragen    aspects_reflection = generate_aspects_reflection(conversation_summaries)    # Erstellen eines Prompts, das die Reflexionen der Teilnehmer und die Gruppendynamik umfasst    prompt = "Based on the following aspects reflections, provide deeper reflections about each chat participant:\n\n"    prompt += aspects_reflection    prompt += "\n\nDeeper Reflections:"    return prompt# Decision whether a member should have a conversation or notdef decide_about_conversation(members):    # Ensure at least two members are selected for conversation    selected_members = random.sample(members, k=3)    return selected_membersdef get_reflections(conversation_summaries):    prompt = generate_reflection_prompt(conversation_summaries)    response = openai.Completion.create(        model="gpt-3.5-turbo-instruct",        prompt=prompt,        max_tokens=400    )    return response.choices[0].text.strip()# Function that simulates the conversation for a memberdef conduct_conversation_for_member(member):    # Here your existing code for conversation simulation would come    print(f"Conversation simulation for {member}")# Main function that is executed dailydef daily_conversation_decision():    today = datetime.date.today()    shuffled_members = members.copy()  # Create a copy of the member list    random.shuffle(shuffled_members)   # Shuffle the list    participating_members = shuffled_members[:3]  # Select the first two members    for member in participating_members:        print(f"Today, on {today}, {member} should have a conversation.")        conduct_conversation_for_member(member)    for member in set(members) - set(participating_members):        print(f"Today, on {today}, {member} should not have a conversation.")def generate_summary_prompt(conversation_history):    conversation_text = "\n".join(conversation_history)    prompt = f"Summary of the following conversation:\n{conversation_text}\n\nSummary:"    return promptdef get_conversation_summary(conversation_history):    prompt = generate_summary_prompt(conversation_history)    response = openai.Completion.create(        model="gpt-3.5-turbo-instruct",        prompt=prompt,        max_tokens=500    )    summary = response.choices[0].text.strip()    return summary# generates suggestions to improve responses based on high-level-insights (get_reflections)def generate_improvement_suggestions_with_openai(conversation_summaries):    # running the method get_reflections() and further    reflection = get_reflections(conversation_summaries)    # generate suggestions    prompt = "Generate improvement suggestions for the responses of each person that participated:\n\n" + reflection    # using system as role to later on add it to the prompts in line 208    # if that does not work it will get anyway added later on in conversation_summary and in the database    response = openai.ChatCompletion.create(        model="gpt-3.5-turbo",        messages=[            {"role": "system", "content": prompt}        ]    )    improvement_suggestions = response.choices[0].message.content    print("suggestions: ")    print(prompt)    return improvement_suggestionsdef limit_response_length(response, max_length=200):    return response if len(response) <= max_length else response[:max_length] + "..."if __name__ == "__main__":    # run daily decision method    daily_conversation_decision()# Read system description from filewith open('prompt_templates/system_desc', "r") as f:    system_prompt = f.read()# Replace placeholders in system prompt with actual topic and number of roundssystem_prompt = system_prompt.replace("<topic>", topic)system_prompt = system_prompt.replace("<Num_Rounds>", str(num_rounds))system_prompt = system_prompt.replace("<members>", str(members))# Read the persona description from a filewith open('prompt_templates/persona_desc', "r") as f:    persona_prompt = f.read()# Read specific instructions for Person Awith open('prompt_templates/instructions/Elias/v1', "r") as f:    instruction_personA = f.read()# Read specific instructions for Person Bwith open('prompt_templates/instructions/Leon/v1', "r") as f:    instruction_personB = f.read()# Read specific instructions for Person Cwith open('prompt_templates/instructions/Matteo/v1', "r") as f:    instruction_personC = f.read()# Create prompts for Person A, Person B and Person C by combining the persona description and specific instructionspersonA_prompt = persona_prompt.replace('<Role>', 'Elias').replace('<Instruction>', instruction_personA)personB_prompt = persona_prompt.replace('<Role>', 'Leon').replace('<Instruction>', instruction_personB)personC_prompt = persona_prompt.replace('<Role>', 'Matteo').replace('<Instruction>', instruction_personC)# Add the system prompt to the prompts for Person A, Person B and Person CpersonA_prompt = system_prompt + '\n' + personA_prompt + '\n'personB_prompt = system_prompt + '\n' + personB_prompt + '\n'personC_prompt = system_prompt + '\n' + personC_prompt + '\n'# Start of conversation roundsround = 1messageList= []id_list =[]counter=0while round <= num_rounds:    # Generate a response from Person A    res = openai.ChatCompletion.create(        model="gpt-3.5-turbo",        messages=[            {"role": "user", "content": personA_prompt + '\n you respond to the last response of Leon or PersonC. After generating, it should display the topic: [topic]; Rounds to go before conversation ends: ' + str(num_rounds - round)},        ]    )    # Update the prompts with the response from Person A    responseA = res.choices[0].message.content    personA_prompt = personA_prompt + '\n\n' + responseA    personB_prompt = personB_prompt + '\n\n' + responseA    personC_prompt = personC_prompt + '\n\n' + responseA    # add response A to provisionally memory stream    conversation_history.append(f"PersonA: {responseA}")    id_list.append(str(counter))    counter = counter + 1    # Output the response from Person A    print(f"Round {round} - PersonA's response:")    print(responseA)    # Generate a response from Person B    res = openai.ChatCompletion.create(        model="gpt-3.5-turbo",        messages=[            {"role": "user", "content": personB_prompt + '\n If you start the conversation, generate a statement once of the topic.  On the other hand, youre based on the statement of Elias which points out a positive aspect about the topic that also invalidates the statement of Elias or PersonC. Rounds to go before conversation ends: ' + str(num_rounds - round)},        ]    )    # Update the prompts with the response from Person B    responseB = res.choices[0].message.content    personA_prompt = personA_prompt + '\n\n' + responseB    personB_prompt = personB_prompt + '\n\n' + responseB    personC_prompt = personC_prompt + '\n\n' + responseB    # add response B to provisionally memory stream    conversation_history.append(f"PersonB: {responseB}")    id_list.append(str(counter))    counter = counter + 1    # Output the response from Person B    print(f"Round {round} - PersonB's response:")    print(responseB)    # Generate a response from Person C    res = openai.ChatCompletion.create(        model="gpt-3.5-turbo",        messages=[            {"role": "user", "content": personC_prompt + '\n If you start the conversation, generate a statement once of the topic.  On the other hand, youre based on the conversation of peronA or Leon so far. Rounds to go before conversation ends: ' + str(num_rounds - round)},        ]    )    # Update the prompts with the response from Person C    responseC = res.choices[0].message.content    personA_prompt = personA_prompt + '\n\n' + responseC    personB_prompt = personB_prompt + '\n\n' + responseC    personC_prompt = personC_prompt + '\n\n' + responseC    # add response C to provisionally memory stream    conversation_history.append(f"PersonC: {responseC}")    id_list.append(str(counter))    counter = counter + 1    # Output the response from Person B    print(f"Round {round} - PersonC's response:")    print(responseC)    # Request user feedback and integrate it into prompts (nicht ganz sicher, sometimes interrupts the flow and sometimes not)    user_feedback = input("Please provide feedback for the conversation: ")    personA_prompt += f'\n\nUser Feedback for Round {round}: {user_feedback}'    personB_prompt += f'\n\nUser Feedback for Round {round}: {user_feedback}'    personC_prompt += f'\n\nUser Feedback for Round {round}: {user_feedback}'    # Proceed to the next round    round += 1# After all conversation rounds are completedsummary = get_conversation_summary(conversation_history)ratings = get_member_ratings(conversation_history, members)# Prepare data for reflectionsconversation_summaries = [{"participant": member, "conversation": summary} for member in members]# Generate reflectionsreflections = get_reflections(conversation_summaries)# generate suggestions out of reflectionssuggestions = generate_improvement_suggestions_with_openai(conversation_summaries)# add reflection to conversation_history (provisionally memory stream)conversation_history.append(f"Reflections: {reflections}")# add suggestion to provisionally memory streamconversation_history.append(f"Suggestions: {suggestions}")# Display summary, ratings, reflections and suggestionprint("\nSummary:\n", summary)print("\nRatings of overall contributions with explanations:")for member, (rating, explanation) in ratings.items():    if rating is not None:        print(f"{member}: {rating} - {explanation}")    else:        print(f"{member}: No rating available")print("\nReflections on the conversations:")print(reflections)print("\nSuggestions based on the Reflections and further more:")print(suggestions)# Wir sind uns nicht sicher ob vor oder nachher besser ist, i guess better than nothingconversation_history.append(f"Reflections: {reflections}")# Request of database (specifically)results = collection.query(query_texts=["flashcards"], n_results=10)# iterate trough outer listfor inner_list in results["documents"]:    # iterate trough every document in inner list    for document in inner_list:        print(document)# After completing all conversation rounds (adding to database)if conversation_history:    try:        for index, entry in enumerate(conversation_history):            document_id = f"conversation_entry_{index}"            collection.add(documents=[entry], ids=[document_id])    except Exception as e:        print(f"Error adding the entry to the database: {e}")